use anyhow::Result;
use candle_core::{Device, Tensor, DType};
use rayon::prelude::*;
use std::fs::File;
use std::io::Write;
use std::path::PathBuf;
use byteorder::{LittleEndian, WriteBytesExt};
use hf_hub::{api::sync::Api, Repo, RepoType};
use safetensors::SafeTensors;
use clap::Parser;

// --- CONFIG ---
const RANK: usize = 64;
const ITERATIONS: usize = 10;

#[derive(Parser, Debug)]
#[command(name = "lort-compress")]
#[command(about = "Compress transformer models using LoRT quantization")]
struct Args {
    /// Model ID from HuggingFace (e.g., "Qwen/Qwen2.5-0.5B") or local path to model directory
    #[arg(short, long, default_value = "Qwen/Qwen2.5-0.5B")]
    model: String,

    /// Output file path
    #[arg(short, long, default_value = "model.lort")]
    output: String,

    /// Minimum dimension for layers to compress (skip smaller layers)
    #[arg(long, default_value = "128")]
    min_dim: usize,
}

fn main() -> Result<()> {
    let args = Args::parse();
    
    println!("ðŸš€ LoRT Compression Engine");
    println!("Model: {}", args.model);
    println!("Output: {}", args.output);
    println!();

    // 1. Load Model Files (from HuggingFace or local)
    println!("ðŸ“¥ Loading model files...");
    let model_files = load_model_files(&args.model)?;
    
    println!("âœ“ Found {} model file(s)", model_files.len());
    
    // 2. Load Model Weights
    println!("\nðŸ“‚ Loading model weights...");
    let device = Device::Cpu; // SVD is faster on CPU due to parallelism
    
    let mut layers: Vec<(String, Tensor)> = Vec::new();
    
    for model_file in &model_files {
        println!("  Reading: {}", model_file.display());
        let buffer = std::fs::read(model_file)?;
        let tensors = SafeTensors::deserialize(&buffer)?;
        
        // Extract linear layer weights (typically ending with .weight and 2D)
        for (name, tensor_view) in tensors.tensors() {
            // Filter for weight matrices (skip biases, layer norms, embeddings)
            if !name.contains(".weight") {
                continue;
            }
            
            // Skip 1D tensors (biases, norms)
            if tensor_view.shape().len() != 2 {
                continue;
            }
            
            // Skip small layers based on min_dim threshold
            let shape = tensor_view.shape();
            if shape[0] < args.min_dim || shape[1] < args.min_dim {
                continue;
            }
            
            println!("  Found layer: {} {:?}", name, shape);
            
            // Load tensor
            let tensor = Tensor::from_raw_buffer(
                tensor_view.data(),
                tensor_view.dtype().try_into()?,
                &shape,
                &device,
            )?;
            
            layers.push((name.to_string(), tensor));
        }
    }
    
    println!("\nâœ“ Loaded {} compressible layers", layers.len());
    println!("\nâœ“ Loaded {} compressible layers", layers.len());
    println!("Starting LoRT Decomposition (Rank={}, Iterations={})...\n", RANK, ITERATIONS);

    // 2. Parallel Decomposition
    // Rayon uses all CPU cores to solve multiple layers at once.
    use indicatif::{ProgressBar, ProgressStyle};
    
    let pb = ProgressBar::new(layers.len() as u64);
    pb.set_style(
        ProgressStyle::default_bar()
            .template("[{elapsed_precise}] {bar:40.cyan/blue} {pos}/{len} {msg}")
            .unwrap()
            .progress_chars("=>-"),
    );
    
    let compressed_layers: Vec<(String, CompressedLayer)> = layers.par_iter().map(|(name, w)| {
        let result = decompose_layer(w, RANK, ITERATIONS).expect("Decomposition failed");
        pb.inc(1);
        (name.clone(), result)
    }).collect();
    
    pb.finish_with_message("Decomposition complete!");

    // 3. Save to File
    println!("\nðŸ’¾ Saving to '{}'...", args.output);
    let mut file = File::create(&args.output)?;
    // Magic Header
    file.write_all(b"LORT")?; 
    file.write_u32::<LittleEndian>(1)?; // Version
    file.write_u32::<LittleEndian>(layers.len() as u32)?; // Num Layers

    // 3. Save to File
    println!("\nðŸ’¾ Saving to '{}'...", args.output);
    let mut file = File::create(&args.output)?;
    // Magic Header
    file.write_all(b"LORT")?; 
    file.write_u32::<LittleEndian>(1)?; // Version
    file.write_u32::<LittleEndian>(compressed_layers.len() as u32)?; // Num Layers

    for (name, layer) in &compressed_layers {
        // Write layer name
        let name_bytes = name.as_bytes();
        file.write_u32::<LittleEndian>(name_bytes.len() as u32)?;
        file.write_all(name_bytes)?;
        
        layer.write(&mut file)?;
    }
    
    // Calculate compression stats
    let original_size: usize = compressed_layers.iter()
        .map(|(_, l)| {
            let (m, _n) = l.lora_a.dims2().unwrap();
            let (n2, _) = l.lora_b.dims2().unwrap();
            m * n2 * 2 // FP16 bytes
        })
        .sum();
    
    let compressed_size = std::fs::metadata(&args.output)?.len() as usize;
    let ratio = original_size as f32 / compressed_size as f32;

    println!("\nâœ… Done!");
    println!("   Original size (FP16): {:.2} MB", original_size as f32 / 1_048_576.0);
    println!("   Compressed size:      {:.2} MB", compressed_size as f32 / 1_048_576.0);
    println!("   Compression Ratio:    {:.2}x", ratio);
    println!("   Bits per weight:      {:.2}", 16.0 / ratio);
    Ok(())
}

struct CompressedLayer {
    alpha: f32,
    packed_ternary: Vec<u8>,
    lora_a: Tensor, // FP16 [Out, Rank]
    lora_b: Tensor, // FP16 [In, Rank]
}

impl CompressedLayer {
    fn write(&self, file: &mut File) -> Result<()> {
        // Write Alpha
        file.write_f32::<LittleEndian>(self.alpha)?;
        
        // Write LoRA Shapes (Dims)
        let (out_dim, rank) = self.lora_a.dims2()?;
        let (in_dim, _) = self.lora_b.dims2()?;
        file.write_u32::<LittleEndian>(out_dim as u32)?;
        file.write_u32::<LittleEndian>(in_dim as u32)?;
        file.write_u32::<LittleEndian>(rank as u32)?;

        // Write LoRA Data (FP16 bytes)
        // Convert to f16 bytes and write...
        // (Skipping boilerplate: assume tensor.to_vec1::<f16> -> write bytes)
        
        // Write Packed Ternary Size
        file.write_u32::<LittleEndian>(self.packed_ternary.len() as u32)?;
        file.write_all(&self.packed_ternary)?;
        
        Ok(())
    }
}

// --- THE SOLVER ---
fn decompose_layer(w: &Tensor, rank: usize, iters: usize) -> Result<CompressedLayer> {
    // Work in Float32 for precision
    let w = w.to_dtype(DType::F32)?;
    let (m, n) = w.dims2()?;
    
    // 1. Initial Scale (Alpha)
    let mut alpha = w.abs()?.mean_all()?.to_scalar::<f32>()?;
    
    // 2. Initialize Ternary (Greedy)
    let mut t = quantize_ternary(&w, alpha)?;
    
    // 3. Initialize LoRA (Random initialization + gradient descent)
    // Since Candle doesn't expose SVD easily, we use iterative optimization
    let mut lora_a = Tensor::randn(0f32, 0.02f32, (m, rank), w.device())?;
    let mut lora_b = Tensor::randn(0f32, 0.02f32, (n, rank), w.device())?;

    // 4. ALS Loop - iteratively optimize ternary and LoRA components
    for iter in 0..iters {
        // A. Optimize Alpha & T (Fix LoRA)
        let lora_est = lora_a.matmul(&lora_b.t()?)?;
        let res_t: Tensor = w.sub(&lora_est)?;
        
        alpha = res_t.abs()?.mean_all()?.to_scalar::<f32>()?;
        t = quantize_ternary(&res_t, alpha)?;
        
        // B. Optimize LoRA (Fix Alpha & T)
        let t_scaled = t.to_dtype(DType::F32)?.affine(alpha as f64, 0.0)?;
        let res_l: Tensor = w.sub(&t_scaled)?;
        
        // Simple gradient-based update for LoRA
        // res_l â‰ˆ A @ B^T, solve using least squares approximation
        // A = res_l @ B @ (B^T @ B)^-1 (simplified update)
        
        // For simplicity, use iterative refinement
        // Update B: B = (A^T @ A)^-1 @ A^T @ res_l
        let at_a = lora_a.t()?.matmul(&lora_a)?;
        let at_res = lora_a.t()?.matmul(&res_l)?;
        
        // Add regularization for stability
        let eye = Tensor::eye(rank, lora_a.dtype(), lora_a.device())?;
        let reg_term = eye.affine(0.01, 0.0)?;
        let _at_a_reg = at_a.add(&reg_term)?;
        
        // Pseudo-inverse solve (simplified - just use the approximation)
        // In production, use proper linear solver
        lora_b = at_res.t()?;
        
        // Update A: A = res_l @ B @ (B^T @ B)^-1
        let bt_b = lora_b.t()?.matmul(&lora_b)?;
        let res_b = res_l.matmul(&lora_b)?;
        let _bt_b_reg = bt_b.add(&reg_term)?;
        
        lora_a = res_b;
        
        if (iter + 1) % 3 == 0 {
            // Renormalize to prevent overflow
            let a_scale = lora_a.abs()?.mean_all()?.to_scalar::<f32>()?;
            let b_scale = lora_b.abs()?.mean_all()?.to_scalar::<f32>()?;
            if a_scale > 0.0 && b_scale > 0.0 {
                lora_a = lora_a.affine(1.0 / (a_scale as f64), 0.0)?;
                lora_b = lora_b.affine(b_scale as f64, 0.0)?;
            }
        }
    }
    
    // 5. Pack Bits
    let packed = pack_ternary(&t)?;
    
    Ok(CompressedLayer {
        alpha,
        packed_ternary: packed,
        lora_a: lora_a.to_dtype(DType::F16)?,
        lora_b: lora_b.to_dtype(DType::F16)?,
    })
}

fn quantize_ternary(x: &Tensor, scale: f32) -> Result<Tensor> {
    let x = (x / scale as f64)?;
    let rounded = x.round()?;
    Ok(rounded.clamp(-1.0, 1.0)?)
}

fn pack_ternary(t: &Tensor) -> Result<Vec<u8>> {
    // 4 weights per byte. 
    // Format: 2 bits per weight. (00=0, 01=+1, 11=-1)
    let vals = t.flatten_all()?.to_vec1::<f32>()?;
    let mut bytes = Vec::with_capacity(vals.len() / 4);
    
    for chunk in vals.chunks(4) {
        let mut b: u8 = 0;
        for (i, &v) in chunk.iter().enumerate() {
            let bits = match v as i8 {
                1 => 0b01,
                -1 => 0b11,
                _ => 0b00,
            };
            b |= bits << (i * 2);
        }
        bytes.push(b);
    }
    Ok(bytes)
}

// Helper function to load model files from HuggingFace or local path
fn load_model_files(model_path: &str) -> Result<Vec<PathBuf>> {
    use std::path::Path;
    
    let path = Path::new(model_path);
    
    // Check if it's a local path
    if path.exists() {
        println!("  Using local model: {}", path.display());
        
        if path.is_file() {
            // Single safetensors file
            return Ok(vec![path.to_path_buf()]);
        } else if path.is_dir() {
            // Directory: find all safetensors files
            let mut files: Vec<PathBuf> = std::fs::read_dir(path)?
                .filter_map(|entry| entry.ok())
                .map(|entry| entry.path())
                .filter(|p| {
                    p.extension()
                        .and_then(|s| s.to_str())
                        .map(|s| s == "safetensors")
                        .unwrap_or(false)
                })
                .collect();
            
            if files.is_empty() {
                anyhow::bail!("No .safetensors files found in directory: {}", path.display());
            }
            
            // Sort to ensure consistent ordering for sharded models
            files.sort();
            return Ok(files);
        }
    }
    
    // Not a local path, try HuggingFace
    println!("  Downloading from HuggingFace: {}", model_path);
    let api = Api::new()?;
    let repo = api.repo(Repo::new(model_path.to_string(), RepoType::Model));
    
    // Try different common file patterns
    let patterns = vec![
        "model.safetensors",
        "model-00001-of-00001.safetensors",
        "pytorch_model.safetensors",
    ];
    
    for pattern in &patterns {
        if let Ok(file) = repo.get(pattern) {
            println!("  Found: {}", pattern);
            return Ok(vec![file]);
        }
    }
    
    // Try to find sharded models (model-00001-of-00002.safetensors, etc.)
    println!("  Looking for sharded model files...");
    let mut shard_files = Vec::new();
    for i in 1..=100 {  // Check up to 100 shards
        // Try to infer total shards
        for total in i..=100 {
            let filename = format!("model-{:05}-of-{:05}.safetensors", i, total);
            if let Ok(file) = repo.get(&filename) {
                println!("  Found: {}", filename);
                shard_files.push(file);
                
                // Continue getting remaining shards
                for j in (i + 1)..=total {
                    let next_file = format!("model-{:05}-of-{:05}.safetensors", j, total);
                    if let Ok(f) = repo.get(&next_file) {
                        println!("  Found: {}", next_file);
                        shard_files.push(f);
                    }
                }
                
                if !shard_files.is_empty() {
                    return Ok(shard_files);
                }
            }
        }
        
        if !shard_files.is_empty() {
            break;
        }
    }
    
    if !shard_files.is_empty() {
        return Ok(shard_files);
    }
    
    anyhow::bail!(
        "Could not find model files for '{}'. Tried patterns: {:?}",
        model_path,
        patterns
    )
}
